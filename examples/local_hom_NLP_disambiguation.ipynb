{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Homology NLP Use Cases: unsupervised word disambiguation\n",
    "\n",
    "In this tutorial we apply **local homology** to study natural language processing (NLP) data. \n",
    "\n",
    "## The context\n",
    "\n",
    "Most modern machine learning techniques dealing with NLP data need to preprocess text data and trasform them into more standard objects: arrays. The process of transforming a word (or token, i.e. a word stripped out of its ending) into an array is called *word embedding*. \n",
    "\n",
    "There are of course more general techniques than simply transforming the single tokens: sentence embedding is one of the generalisations. However, for the purpose of this notebook, we will stick to word embeddings and in particular we will use a technique called [word2vec](https://en.wikipedia.org/wiki/Word2vec).\n",
    "\n",
    "The disadvantage of any word embedding techniques is that the same token is mapped to the same array. Hence, if two words have the same written form but different meanings (i.e., *homographic words*), such words will anyway be mapped to the same array!\n",
    "\n",
    "## The task \n",
    "\n",
    "Given the above introduction, our task is to *disambiguate words*, i.e. finding a way to differentiate homographic words by their meaning. Our approach consists of analysing the whole sentence in which the word appears and try to deduce the meaning of the word from its context (i.e. neighbouring words). The core idea of our proposal is based on the algebro-topological description of the space of word embeddings.\n",
    "\n",
    "## The main idea\n",
    "\n",
    "We are structuring our analysis on the assumptions that are clearly explained in [this paper](https://arxiv.org/pdf/2011.09413.pdf). In few words, the idea is that a word with multiple meaning sits on the sigular loci of the stratified space of the word embeddings. This sentence is in truth not formally correct, as there is yet no clear notion of what is the canonical topology in the word embedding space; nonetheless, the intuition behind these concepts can be explained pictorially:\n",
    "\n",
    "![sing](images/local_singularity.png)\n",
    "\n",
    "\n",
    "\n",
    "This picture represents the local shape of the word-embedding space: the context words are located over the four cones tipping at the word `mole` (the singularity). Hence, a sentence from a thriller containing the word `mole` would most probably be located on the north-west branch, and so on.\n",
    "\n",
    "## The goal of our exploration\n",
    "\n",
    "We would first like to stress that this notebook is merely exploratory and that there is no aim at making it a fully fledge ML pipeline for word disambiguation. \n",
    "We would really like to understand if the shape of the cones around the singularity (i.e. the `mole` point) can be distinguished with **local homology**: if that is the case, then the geometric shape of a sentence lying on the word embedding stratified space correlates with the meaning of a word! This entails a new disambiguation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.local_homology.simplicial import KNeighborsLocalVietorisRips\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we showcase how to use local homology to disambiguate words. In particular, we focus our analysis to disambiguate the word \"note\": either a musical frequency, a short text, the verb, or money.\n",
    "\n",
    "## The dataset and its preprocessing\n",
    "\n",
    "The dataset contains many occurrences of the word \"note\" with the meanings stated above. The format is `.xml`, out of which we extract the plain text and do the standard preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "with open(\"data/note.n.xml\",\"r\") as f:\n",
    "    content = f.read()\n",
    "    \n",
    "    # split the sentences\n",
    "    temp_list = content.split(\"<note.n.\")  \n",
    "    \n",
    "    # remove stopwords\n",
    "    list_of_text = list(map(remove_stopwords, temp_list))  \n",
    "    \n",
    "    # make lower case\n",
    "    list_of_text = list(map(stem, temp_list))  \n",
    "    refined_list = [list_of_text[i][1+len(str(i)):-12 - len(str(i))] for i in range(2,len(list_of_text))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we use the `word2vec` technique to vectorise each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the words from the sentences\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class PreprocessingText(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"A basic class to transform a list of sentences (strings)\n",
    "    into a list of arrays, each with 2 dimensions: (n_words, dim_emb_space)\n",
    "    \n",
    "    Note that the output is a list of arrays, each of two dimensions: the\n",
    "    first dimension is the number of words in a sentence, while the \n",
    "    second dimension the word embedding dimension.\n",
    "    \n",
    "    The ``item``parameter is useful to select, for the transform, which item\n",
    "    to select.\n",
    "    Set ``item = None`` to get the whole list\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_size=30, window=5, min_count=1, \n",
    "                 workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        all_words_in_sentences = list(map(str.split, X))\n",
    "        self.word2vec = Word2Vec(sentences=all_words_in_sentences, \n",
    "                            vector_size=self.vector_size, \n",
    "                            window=self.window, \n",
    "                            min_count=self.min_count, \n",
    "                            workers=self.workers,\n",
    "                            seed=11\n",
    "                           )\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        all_words_in_sentences = list(map(str.split, X))\n",
    "        list_of_vect_sentences = [self.word2vec.wv[all_words_in_sentences[i]] for i in range(len(all_words_in_sentences))]\n",
    "        return list_of_vect_sentences\n",
    "\n",
    "pt = PreprocessingText()\n",
    "pt.fit(refined_list)\n",
    "list_of_vect_sentences = pt.fit_transform(refined_list) # all vectorized sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exploratory results\n",
    "\n",
    "Here below we see a couple of sentences containing the word \"note\" with the different meanings described above.\n",
    "\n",
    "We display the sentence as well as the `modified_persistence_entropy` of the sentence, and then visualize their embeddings with umap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better interpret the persistence diagram, we introduce some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams.features import PersistenceEntropy\n",
    "\n",
    "class ModifiedPersistenceEntropy(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"This class respects the sklearn paradigm and is useful to \n",
    "    vectorize the persistence diagrams coming out of the local\n",
    "    homology class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pe = PersistenceEntropy()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.pe.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return 2**self.pe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a preprocessed sentence where \"note\" is used as a verb\n",
    "print(refined_list[0])\n",
    "\n",
    "lh = KNeighborsLocalVietorisRips(n_neighbors=(5, 15),\n",
    "                                 homology_dimensions=(0, 1),\n",
    "                                 collapse_edges=True, \n",
    "                                 n_jobs = -1)\n",
    "lh.fit(list_of_vect_sentences[0])\n",
    "ModifiedPersistenceEntropy().fit_transform(lh.transform(pt.word2vec.wv[\"note\"].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of preprocessed sentence where \"note\" refers to music\n",
    "print(refined_list[1])\n",
    "\n",
    "lh = KNeighborsLocalVietorisRips(n_neighbors=(5, 15),\n",
    "                                 homology_dimensions=(0, 1),\n",
    "                                 collapse_edges=True, \n",
    "                                 n_jobs = -1)\n",
    "lh.fit(list_of_vect_sentences[1])\n",
    "ModifiedPersistenceEntropy().fit_transform(lh.transform(pt.word2vec.wv[\"note\"].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the word2vec embedding of two different sentences, using umap dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports that will help us visualize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "def plotting_the_embedding(i, string):\n",
    "    \"\"\"this function displays the word embedding space reduced\n",
    "    to two dimensions by the UMAP algorithm. In yellow the word\n",
    "    `note` is highlighted. \n",
    "    \"\"\"\n",
    "    value = None\n",
    "    # for loop to find the instance of note\n",
    "    for k in range(len(list_of_vect_sentences[i])):\n",
    "        if (list_of_vect_sentences[i][k] == pt.transform((\"note\",))[0]).all():\n",
    "            value = k\n",
    "    temp = np.zeros((len(list_of_vect_sentences[i])))\n",
    "    temp[value] = 1\n",
    "\n",
    "    reducer = umap.UMAP()\n",
    "\n",
    "    scaled_point_cloud = StandardScaler().fit_transform(list_of_vect_sentences[i])\n",
    "\n",
    "    embedding = reducer.fit_transform(scaled_point_cloud)\n",
    "\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1], c = temp)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('Use of \"note\" as a ' + string, fontsize=24)\n",
    "    \n",
    "    # Example of sentence with the word \"note\"\n",
    "    print(\"Preprocessed sentence: \")\n",
    "    print(refined_list[i])\n",
    "    lh.fit(list_of_vect_sentences[i])\n",
    "\n",
    "    print(\"First and second Betti numbers:\")\n",
    "    print(ModifiedPersistenceEntropy().fit_transform(lh.transform(np.array(pt.transform((\"note\",))[0], dtype=float))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note as a verb\n",
    "i=0 \n",
    "plotting_the_embedding(i, \"verb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musical note\n",
    "i=1\n",
    "plotting_the_embedding(i, \"musical note\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique seems promising: the results above show a clear distinction between the local shape of the neighborhoods for the word \"note\" with different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small scale statistical exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at 30 sentences where the word \"note\" takes different meanings which we have hand labelled.\n",
    "For each 30 sentences, we look at the 0th and 1st dimension local homology around note and plot the obtained 2 dimensional pointcloud coloured by the meaning of \"note\" in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_emb = pt.word2vec.wv[\"note\"]\n",
    "note_loc_hom = []\n",
    "for i in range(30):\n",
    "    lh = KNeighborsLocalVietorisRips(n_neighbors=(5, 15),\n",
    "                                 homology_dimensions=(0, 1),\n",
    "                                 collapse_edges=True, \n",
    "                                 n_jobs = -1)\n",
    "    sentence_emb = list_of_vect_sentences[i]\n",
    "    lh.fit(sentence_emb)\n",
    "    note_loc_hom.append(ModifiedPersistenceEntropy().fit_transform(lh.transform(note_emb.reshape(1, -1)))[0])\n",
    "\n",
    "note_loc_hom = np.array(note_loc_hom)\n",
    "\n",
    "plt.scatter(\n",
    "    note_loc_hom[:, 0],\n",
    "    note_loc_hom[:, 1],\n",
    "    c = [0, 1, 2, 2, 1, 1, 2, 2, 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 3, 3, 3, 2, 1, 1, 3, 2, 2, 1])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Local dimension around \"note\"', fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The method seems promising! In particular, the meaning \"money\" (in yellow) seems to have very varying local dimensions, whereas the other classes seem to be more clustered together. However a lot more work is needed: especially, systematising the anylsis, finding the proper vectorisation of local homology, etc...\n",
    "\n",
    "We really hope that this notebook will tinkle your attention and suggest you new relevant research dierctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Unique pipeline\n",
    "\n",
    "All the steps above can be merged together into a `Pipeline`: note however the trick we used with the `item` parameter in the `PreprocessingText` class (due to the non-standard dimensions of text embeddings). With the next cell we conclude our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"preprocessing\", PreprocessingText()),\n",
    "                 (\"local homology\", KNeighborsLocalVietorisRips(n_neighbors=(5, 15),\n",
    "                                 homology_dimensions=(1,2),\n",
    "                                 collapse_edges=True, \n",
    "                                 n_jobs = -1)), \n",
    "                (\"vectorizer\", ModifiedPersistenceEntropy())])\n",
    "\n",
    "# The training dataset, where each item is composed of word embeddings. \n",
    "# Here below we select only the first item:\n",
    "X_train = refined_list\n",
    "\n",
    "# this is a simple example to test the disambiguation of the word \"note\"\n",
    "X_test = (\"note\", )  \n",
    "\n",
    "pipe.fit(X_train)\n",
    "\n",
    "pipe.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
