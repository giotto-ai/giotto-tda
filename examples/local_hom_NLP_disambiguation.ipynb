{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Homology NLP Use Cases: Unsupervised text disambiguation\n",
    "\n",
    "In this tutorial apply local homology to study natural language processing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gtda.local_homology.simplicial import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent research have been looking at the the role of local homology in Natural Language Processing, and particularly to the task of text dissamgibuating'. Here we showcase a method that can be useful to distinguish occurences of the word \"note\" when referring to a musical \"note\", versus when used as a word referring to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "f = open(\"data/note.n.xml\",\"r\")\n",
    "content = f.read()\n",
    "temp_list = content.split(\"<note.n.\")\n",
    "list_of_text = list(map(remove_stopwords,temp_list)) # remove stopwords\n",
    "list_of_text = list(map(stem, temp_list)) # make lower case\n",
    "refined_list = [list_of_text[i][1+len(str(i)):-12 - len(str(i))] for i in range(2,len(list_of_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to interpret the persistence diagram, we introduce some helper functions:\n",
    "from gtda.diagrams.features import PersistenceEntropy\n",
    "\n",
    "def modified_persistence_entropy(diags):\n",
    "    \"\"\" This is a custom vectorizer, similar to functions\n",
    "    in gtda.diagrams.features. Inputs a sequence of persistence\n",
    "    diagrams, and outputs a sequence of vectors\"\"\"\n",
    "    return 2**PersistenceEntropy().fit_transform(diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentences and vectorize them\n",
    "all_words_in_sentences = list(map(str.split,refined_list))\n",
    "word2vec = Word2Vec(sentences=all_words_in_sentences, vector_size=30, window=5, min_count=1, workers=4)\n",
    "\n",
    "# list of array with vectorized snipet\n",
    "list_of_vect_sentences = [word2vec.wv[all_words_in_sentences[i]] for i in range(len(all_words_in_sentences))]\n",
    "\n",
    "# initialize the local homology transformer\n",
    "lh = KNeighborsLocalVietorisRips(n_neighbors=(5, 15),\n",
    "                                 homology_dimensions=(1,2),\n",
    "                                 collapse_edges=True, \n",
    "                                 n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a preprocessed sentence where \"note\" is used as a verb\n",
    "print(refined_list[0])\n",
    "lh.fit(list_of_vect_sentences[0])\n",
    "modified_persistence_entropy(lh.transform(np.array([word2vec.wv[\"note\"]], dtype=float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of preprocessed sentence where \"note\" refers to music\n",
    "print(refined_list[1])\n",
    "lh.fit(list_of_vect_sentences[1])\n",
    "modified_persistence_entropy(lh.transform(np.array([word2vec.wv[\"note\"]], dtype=float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports that will help us visualize the data\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note as a verb\n",
    "\n",
    "i=0 \n",
    "value = None\n",
    "# for loop to find the instance of note\n",
    "for k in range(len(list_of_vect_sentences[i])):\n",
    "    if (list_of_vect_sentences[i][k] == word2vec.wv[\"note\"]).all():\n",
    "        value = k\n",
    "temp = np.zeros((len(list_of_vect_sentences[i])))\n",
    "temp[value] = 1\n",
    "print(\"Note is at the \" + str(value) + \"th position.\")\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "scaled_point_cloud = StandardScaler().fit_transform(list_of_vect_sentences[i])\n",
    "\n",
    "embedding = reducer.fit_transform(scaled_point_cloud)\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c = temp)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Use of \"note\" as a verb', fontsize=24)\n",
    "\n",
    "# Example of sentence has \"note\" in it used as the verb\n",
    "print(\"Preprocessed sentence: \")\n",
    "print(refined_list[i])\n",
    "lh.fit(list_of_vect_sentences[i])\n",
    "\n",
    "print(\"First and second Betti numbers:\")\n",
    "print(modified_persistence_entropy(lh.transform(np.array([word2vec.wv[\"note\"]], dtype=float))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musical note\n",
    "i=1\n",
    "value=None\n",
    "for k in range(len(list_of_vect_sentences[i])):\n",
    "    if (list_of_vect_sentences[i][k] == word2vec.wv[\"note\"]).all():\n",
    "        value = k\n",
    "temp = np.zeros((len(list_of_vect_sentences[i])))\n",
    "temp[value] = 1\n",
    "\n",
    "print(\"Note is at the \" + str(value) + \"th position.\")\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "scaled_point_cloud = StandardScaler().fit_transform(list_of_vect_sentences[i])\n",
    "\n",
    "embedding = reducer.fit_transform(scaled_point_cloud)\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1], c = temp)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Use of \"note\" referring to music', fontsize=24)\n",
    "\n",
    "i=1\n",
    "# This one uses the musical note\n",
    "print(\"Preprocessed sentence: \")\n",
    "print(refined_list[i])\n",
    "lh.fit(list_of_vect_sentences[i])\n",
    "\n",
    "print(\"First and second Betti numbers:\")\n",
    "print(modified_persistence_entropy(lh.transform(np.array([word2vec.wv[\"note\"]], dtype=float))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
